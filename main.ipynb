{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwrAtGm2HfMWPgvXB8UXFx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_transformers"
      ],
      "metadata": {
        "id": "rMlY7dNFKXKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import collections\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_transformers.tokenization_bert import (BasicTokenizer, whitespace_tokenize)\n",
        "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset"
      ],
      "metadata": {
        "id": "CMWDBuZc1Dvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SquadExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for the Squad dataset.\n",
        "    For examples without an answer, the start and end position are -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 qas_id,\n",
        "                 question_text,\n",
        "                 doc_tokens,\n",
        "                 orig_answer_text=None,\n",
        "                 start_position=None,\n",
        "                 end_position=None):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.orig_answer_text = orig_answer_text\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        s += \"qas_id: %s\" % (self.qas_id)\n",
        "        s += \", question_text: %s\" % (self.question_text)\n",
        "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "        if self.start_position:\n",
        "            s += \", start_position: %d\" % (self.start_position)\n",
        "        if self.end_position:\n",
        "            s += \", end_position: %d\" % (self.end_position)\n",
        "        return s"
      ],
      "metadata": {
        "id": "9u5pw-EC2xOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 unique_id,\n",
        "                 example_index,\n",
        "                 doc_span_index,\n",
        "                 tokens,\n",
        "                 token_to_orig_map,\n",
        "                 token_is_max_context,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 paragraph_len,\n",
        "                 start_position=None,\n",
        "                 end_position=None,):\n",
        "        self.unique_id = unique_id\n",
        "        self.example_index = example_index\n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.paragraph_len = paragraph_len\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position"
      ],
      "metadata": {
        "id": "Cgx8rSFC3lFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_to_squad_example(passage, question):\n",
        "    \"\"\"Convert input passage and question into a SquadExample.\"\"\"\n",
        "\n",
        "    def is_whitespace(c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    paragraph_text = passage\n",
        "    doc_tokens = []\n",
        "    char_to_word_offset = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "\n",
        "    qas_id = 0\n",
        "    question_text = question\n",
        "    start_position = None\n",
        "    end_position = None\n",
        "    orig_answer_text = None\n",
        "\n",
        "    example = SquadExample(\n",
        "        qas_id=qas_id,\n",
        "        question_text=question_text,\n",
        "        doc_tokens=doc_tokens,\n",
        "        orig_answer_text=orig_answer_text,\n",
        "        start_position=start_position,\n",
        "        end_position=end_position)\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "59LwIur27ift"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "    # Because of the sliding window approach taken to scoring documents, a single\n",
        "    # token can appear in multiple documents. E.g.\n",
        "    #  Doc: the man went to the store and bought a gallon of milk\n",
        "    #  Span A: the man went to the\n",
        "    #  Span B: to the store and bought\n",
        "    #  Span C: and bought a gallon of\n",
        "    #  ...\n",
        "    #\n",
        "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "    # want to consider the score with \"maximum context\", which we define as\n",
        "    # the *minimum* of its left and right context (the *sum* of left and\n",
        "    # right context will always be the same, of course).\n",
        "    #\n",
        "    # In the example the maximum context for 'bought' would be span C since\n",
        "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "    # and 0 right context.\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index"
      ],
      "metadata": {
        "id": "bzmLW_dJ9Ugv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def squad_examples_to_features(example, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length,cls_token_at_end=False,\n",
        "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=0, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    unique_id = 1000000000\n",
        "    # cnt_pos, cnt_neg = 0, 0\n",
        "    # max_N, max_M = 1024, 1024\n",
        "    # f = np.zeros((max_N, max_M), dtype=np.float32)\n",
        "    example_index = 0\n",
        "    features = []\n",
        "    # if example_index % 100 == 0:\n",
        "    #     logger.info('Converting %s/%s pos %s neg %s', example_index, len(examples), cnt_pos, cnt_neg)\n",
        "\n",
        "    query_tokens = tokenizer.tokenize(example.question_text)\n",
        "\n",
        "    if len(query_tokens) > max_query_length:\n",
        "        query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    for (i, token) in enumerate(example.doc_tokens):\n",
        "        orig_to_tok_index.append(len(all_doc_tokens))\n",
        "        sub_tokens = tokenizer.tokenize(token)\n",
        "        for sub_token in sub_tokens:\n",
        "            tok_to_orig_index.append(i)\n",
        "            all_doc_tokens.append(sub_token)\n",
        "\n",
        "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "    # We can have documents that are longer than the maximum sequence length.\n",
        "    # To deal with this we do a sliding window approach, where we take chunks\n",
        "    # of the up to our max length with a stride of `doc_stride`.\n",
        "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"DocSpan\", [\"start\", \"length\"])\n",
        "    doc_spans = []\n",
        "    start_offset = 0\n",
        "    while start_offset < len(all_doc_tokens):\n",
        "        length = len(all_doc_tokens) - start_offset\n",
        "        if length > max_tokens_for_doc:\n",
        "            length = max_tokens_for_doc\n",
        "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "        if start_offset + length == len(all_doc_tokens):\n",
        "            break\n",
        "        start_offset += min(length, doc_stride)\n",
        "\n",
        "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "        tokens = []\n",
        "        token_to_orig_map = {}\n",
        "        token_is_max_context = {}\n",
        "        segment_ids = []\n",
        "\n",
        "        # CLS token at the beginning\n",
        "        if not cls_token_at_end:\n",
        "            tokens.append(cls_token)\n",
        "            segment_ids.append(cls_token_segment_id)\n",
        "\n",
        "        # Query\n",
        "        for token in query_tokens:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(sequence_a_segment_id)\n",
        "\n",
        "        # SEP token\n",
        "        tokens.append(sep_token)\n",
        "        segment_ids.append(sequence_a_segment_id)\n",
        "\n",
        "        # Paragraph\n",
        "        for i in range(doc_span.length):\n",
        "            split_token_index = doc_span.start + i\n",
        "            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                    split_token_index)\n",
        "            token_is_max_context[len(tokens)] = is_max_context\n",
        "            tokens.append(all_doc_tokens[split_token_index])\n",
        "            segment_ids.append(sequence_b_segment_id)\n",
        "        paragraph_len = doc_span.length\n",
        "\n",
        "        # SEP token\n",
        "        tokens.append(sep_token)\n",
        "        segment_ids.append(sequence_b_segment_id)\n",
        "\n",
        "        # CLS token at the end\n",
        "        if cls_token_at_end:\n",
        "            tokens.append(cls_token)\n",
        "            segment_ids.append(cls_token_segment_id)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(pad_token)\n",
        "            input_mask.append(0 if mask_padding_with_zero else 1)\n",
        "            segment_ids.append(pad_token_segment_id)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                unique_id=unique_id,\n",
        "                example_index=example_index,\n",
        "                doc_span_index=doc_span_index,\n",
        "                tokens=tokens,\n",
        "                token_to_orig_map=token_to_orig_map,\n",
        "                token_is_max_context=token_is_max_context,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                paragraph_len=paragraph_len,\n",
        "                start_position=start_position,\n",
        "                end_position=end_position))\n",
        "        unique_id += 1\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "syf86WFuDifg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFWlsBjKJK9w"
      },
      "outputs": [],
      "source": [
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\",[\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    # When we created the data, we kept track of the alignment between original\n",
        "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
        "    # now `orig_text` contains the span of our original text corresponding to the\n",
        "    # span that we predicted.\n",
        "    #\n",
        "    # However, `orig_text` may contain extra characters that we don't want in\n",
        "    # our prediction.\n",
        "    #\n",
        "    # For example, let's say:\n",
        "    #   pred_text = steve smith\n",
        "    #   orig_text = Steve Smith's\n",
        "    #\n",
        "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
        "    #\n",
        "    # We don't want to return `pred_text` because it's already been normalized\n",
        "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
        "    # our tokenizer does additional normalization like stripping accent\n",
        "    # characters).\n",
        "    #\n",
        "    # What we really want to return is \"Steve Smith\".\n",
        "    #\n",
        "    # Therefore, we have to apply a semi-complicated alignment heuristic between\n",
        "    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n",
        "    # can fail in certain cases in which case we just return `orig_text`.\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = collections.OrderedDict()\n",
        "        for (i, c) in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    # We first tokenize `orig_text`, strip whitespace from the result\n",
        "    # and `pred_text`, and check if they are the same length. If they are\n",
        "    # NOT the same length, the heuristic has failed. If they are the same\n",
        "    # length, we assume the characters are one-to-one aligned.\n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        return orig_text\n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        return orig_text\n",
        "\n",
        "    # We then project the characters in `pred_text` back to `orig_text` using\n",
        "    # the character-to-character alignment.\n",
        "    tok_s_to_ns_map = {}\n",
        "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "owvaBwY2L5Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(example, features, all_results, n_best_size,\n",
        "                max_answer_length, do_lower_case):\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "    _PrelimPrediction = collections.namedtuple( \"PrelimPrediction\",[\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    example_index = 0\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "        result = unique_id_to_result[feature.unique_id]\n",
        "        start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "        end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # We could hypothetically create invalid predictions, e.g., predict\n",
        "                # that the start of the span is in the question. We throw out all\n",
        "                # invalid predictions.\n",
        "                if start_index >= len(feature.tokens):\n",
        "                    continue\n",
        "                if end_index >= len(feature.tokens):\n",
        "                    continue\n",
        "                if start_index not in feature.token_to_orig_map:\n",
        "                    continue\n",
        "                if end_index not in feature.token_to_orig_map:\n",
        "                    continue\n",
        "                if not feature.token_is_max_context.get(start_index, False):\n",
        "                    continue\n",
        "                if end_index < start_index:\n",
        "                    continue\n",
        "                length = end_index - start_index + 1\n",
        "                if length > max_answer_length:\n",
        "                    continue\n",
        "                prelim_predictions.append(\n",
        "                    _PrelimPrediction(\n",
        "                        feature_index=feature_index,\n",
        "                        start_index=start_index,\n",
        "                        end_index=end_index,\n",
        "                        start_logit=result.start_logits[start_index],\n",
        "                        end_logit=result.end_logits[end_index]))\n",
        "    prelim_predictions = sorted(prelim_predictions,key=lambda x: (x.start_logit + x.end_logit),reverse=True)\n",
        "    _NbestPrediction = collections.namedtuple(\"NbestPrediction\",\n",
        "                        [\"text\", \"start_logit\", \"end_logit\",\"start_index\",\"end_index\"])\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "        if len(nbest) >= n_best_size:\n",
        "            break\n",
        "        feature = features[pred.feature_index]\n",
        "        orig_doc_start = -1\n",
        "        orig_doc_end = -1\n",
        "        if pred.start_index > 0:  # this is a non-null prediction\n",
        "            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "            tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "            # De-tokenize WordPieces that have been split off.\n",
        "            tok_text = tok_text.replace(\" ##\", \"\")\n",
        "            tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "            # Clean whitespace\n",
        "            tok_text = tok_text.strip()\n",
        "            tok_text = \" \".join(tok_text.split())\n",
        "            orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "            final_text = get_final_text(tok_text, orig_text,do_lower_case)\n",
        "            if final_text in seen_predictions:\n",
        "                continue\n",
        "\n",
        "            seen_predictions[final_text] = True\n",
        "        else:\n",
        "            final_text = \"\"\n",
        "            seen_predictions[final_text] = True\n",
        "\n",
        "        nbest.append(\n",
        "            _NbestPrediction(\n",
        "                text=final_text,\n",
        "                start_logit=pred.start_logit,\n",
        "                end_logit=pred.end_logit,\n",
        "                start_index=orig_doc_start,\n",
        "                end_index=orig_doc_end))\n",
        "\n",
        "    if not nbest:\n",
        "        nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0,start_index=-1,\n",
        "                end_index=-1))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    for entry in nbest:\n",
        "        total_scores.append(entry.start_logit + entry.end_logit)\n",
        "\n",
        "    probs = _compute_softmax(total_scores)\n",
        "\n",
        "    answer = {\"answer\" : nbest[0].text,\n",
        "               \"start\" : nbest[0].start_index,\n",
        "               \"end\" : nbest[0].end_index,\n",
        "               \"confidence\" : probs[0],\n",
        "               \"document\" : example.doc_tokens\n",
        "             }\n",
        "    return answer"
      ],
      "metadata": {
        "id": "trkkoon7MFxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passage = \"Whether you travel for business or pleasure, you know rheumatoid arthritis is one thing that will always come with you. But you can still go wherever you need or want to go. Make it easier on yourself with these tips. Do your homework. Find out as much as you can about your destination and plan all the details you can ahead of time, including what places you'll go, how you'll get there, and what your travel companions can do when you need a rest. Time it right. Choose a time when you're most likely to feel your best. If you tend to get flares during the heat of the summer or the hustle and bustle of the holidays, for example, try to avoid traveling during those times. Don't rush. Although vacations can be fun and restful, they can also be stressful. Try to plan an extra day at the start of your vacation to prepare and another at the end to rest and recover before you go back to work or regular activities. Ask about immunizations. If you're going to travel overseas, ask your doctor about any vaccines you may need. Remind him of what medicines you take, since some immunizations aren't advised if you take medications that suppress your immune system. Choose the right suitcase. Buy a suitcase or bag with wheels, and push instead of pulling it. Use both hands to take it easy on your hands and shoulders. Pack light. Your bags will be easier to carry. If you find that you must lift your suitcase -- into your car trunk or the overhead bin on a plane, for example -- find someone who can help. Don't forget your health info. Write out a brief medical history and list of medications you take. Include contact information for your primary care doctor and rheumatologist, as well as your health insurance information. Mind your medications. Pack more medicine than you think you'll need, and divide it among your different bags. If one bag is lost, you should still have enough medicine to get by. Leave a copy of your prescriptions at home with a friend or family member. If you lose your medications or are gone longer than expected, have them send you your prescription. Don't just sit there. Sitting for hours in a car, plane, bus, or train can lead to stiff joints. When driving, stop once an hour to stretch and walk. When you fly, take a train, or ride a bus, try to get an aisle seat so you can stretch and get up and walk. Avoid crowds. Avoid standing in long lines and flying in crowded planes. Ask the airline or travel agent about times with the least traffic. Bring a doctor's note. If you use medications that require needles, bring a doctor's note or prescription in case you're asked about them at airport security. Avoid stops. When possible, choose nonstop flights. That way you won't have to walk long distances through unfamiliar airports. Arrange for assistive devices. If you use a wheelchair, label it with your name, address, and destination airport -- and ask that it be loaded \\\"last on/first off.\\\" If you use a cane, you can take it on board with you. You'll need to stow it at takeoff and landing, but you can use it during the flight. Pick your room location. When you make hotel reservations, look for a room on or near the main level so you can skip the stairs. Request a refrigerator. If you take medications that need to be refrigerated, an in-room refrigerator is a must. It can also come in handy if you need a quick snack to take medications or to boost your energy level after a day of sightseeing. Look for amenities. A pool can help you stick to your exercise routine, a hot tub can ease sore joints, and an on-site restaurant or room service is helpful if you don't feel like going out at mealtime.\"\n",
        "question = \"How can I remember my health info if I have to travel with rheumatoid arthritis?\""
      ],
      "metadata": {
        "id": "Or9mBxEhNYBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = input_to_squad_example(passage, question)"
      ],
      "metadata": {
        "id": "4scaG0-aPVGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj.__str__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "AoG7534MPi7g",
        "outputId": "f8396efd-3dba-4d4a-ae27-c383ae44fd0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'qas_id: 0, question_text: How can I remember my health info if I have to travel with rheumatoid arthritis?, doc_tokens: [Whether you travel for business or pleasure, you know rheumatoid arthritis is one thing that will always come with you. But you can still go wherever you need or want to go. Make it easier on yourself with these tips. Do your homework. Find out as much as you can about your destination and plan all the details you can ahead of time, including what places you\\'ll go, how you\\'ll get there, and what your travel companions can do when you need a rest. Time it right. Choose a time when you\\'re most likely to feel your best. If you tend to get flares during the heat of the summer or the hustle and bustle of the holidays, for example, try to avoid traveling during those times. Don\\'t rush. Although vacations can be fun and restful, they can also be stressful. Try to plan an extra day at the start of your vacation to prepare and another at the end to rest and recover before you go back to work or regular activities. Ask about immunizations. If you\\'re going to travel overseas, ask your doctor about any vaccines you may need. Remind him of what medicines you take, since some immunizations aren\\'t advised if you take medications that suppress your immune system. Choose the right suitcase. Buy a suitcase or bag with wheels, and push instead of pulling it. Use both hands to take it easy on your hands and shoulders. Pack light. Your bags will be easier to carry. If you find that you must lift your suitcase -- into your car trunk or the overhead bin on a plane, for example -- find someone who can help. Don\\'t forget your health info. Write out a brief medical history and list of medications you take. Include contact information for your primary care doctor and rheumatologist, as well as your health insurance information. Mind your medications. Pack more medicine than you think you\\'ll need, and divide it among your different bags. If one bag is lost, you should still have enough medicine to get by. Leave a copy of your prescriptions at home with a friend or family member. If you lose your medications or are gone longer than expected, have them send you your prescription. Don\\'t just sit there. Sitting for hours in a car, plane, bus, or train can lead to stiff joints. When driving, stop once an hour to stretch and walk. When you fly, take a train, or ride a bus, try to get an aisle seat so you can stretch and get up and walk. Avoid crowds. Avoid standing in long lines and flying in crowded planes. Ask the airline or travel agent about times with the least traffic. Bring a doctor\\'s note. If you use medications that require needles, bring a doctor\\'s note or prescription in case you\\'re asked about them at airport security. Avoid stops. When possible, choose nonstop flights. That way you won\\'t have to walk long distances through unfamiliar airports. Arrange for assistive devices. If you use a wheelchair, label it with your name, address, and destination airport -- and ask that it be loaded \"last on/first off.\" If you use a cane, you can take it on board with you. You\\'ll need to stow it at takeoff and landing, but you can use it during the flight. Pick your room location. When you make hotel reservations, look for a room on or near the main level so you can skip the stairs. Request a refrigerator. If you take medications that need to be refrigerated, an in-room refrigerator is a must. It can also come in handy if you need a quick snack to take medications or to boost your energy level after a day of sightseeing. Look for amenities. A pool can help you stick to your exercise routine, a hot tub can ease sore joints, and an on-site restaurant or room service is helpful if you don\\'t feel like going out at mealtime.]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}